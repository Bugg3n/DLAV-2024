{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"fPFXjAVFIKnh"},"outputs":[],"source":["import argparse\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","import pickle\n","import platform\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uGxnwhvlwMiI"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SwxcJW9wI9fp"},"outputs":[],"source":["!git clone https://github.com/vita-epfl/DLAV-2024.git\n","path = os.getcwd() + '/DLAV-2024/homeworks/hw2/test_batch'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pZXQTJIKJE_S"},"outputs":[],"source":["# Write the location of the saved weight relative to this notebook. Assume that they are in the same directory\n","### Path to Model Weights \n","with open('drive/MyDrive/Colab Notebooks/softmax_weights.pkl', 'rb') as f:\n","    softmax_weights = pickle.load(f)\n","\n","\n","\n","#pytorch_weights = ..."]},{"cell_type":"markdown","metadata":{"id":"mE6psT_aVPHv"},"source":["**TODO:** Copy your code from the Softmax Notebook to their corresponding function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gHnLX6-oIkWm"},"outputs":[],"source":["def softmax_loss_vectorized(W, X, y):\n","    \"\"\"\n","    Softmax loss function, vectorized version.\n","\n","    Inputs have dimension D, there are C classes, and we operate on minibatches\n","    of N examples.\n","\n","    Inputs:\n","    - W: A numpy array of shape (D, C) containing weights.\n","    - X: A numpy array of shape (N, D) containing a minibatch of data.\n","    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n","    that X[i] has label c, where 0 <= c < C.\n","\n","    Returns a tuple of:\n","    - loss as single float\n","    - gradient with respect to weights W; an array of same shape as W\n","    \"\"\"\n","    # Initialize the loss and gradient to zero.\n","    loss = 0.0\n","    dW = np.zeros_like(W)\n","\n","    # Compute the score for each class\n","    class_scores = X.dot(W)\n","    \n","    # Shift the scores so that the highest value is zero (For increased stability)\n","    class_scores -= np.max(class_scores, axis=1, keepdims=True)\n","    \n","    # Compute the probabilities e^zi/sum(e^z)\n","    probabilities = np.exp(class_scores) / np.sum(np.exp(class_scores), axis=1, keepdims=True)\n","\n","    \n","    # Compute the loss, loss(z)=sum(-log(z))/n\n","    N = X.shape[0]  # Number of datapoints\n","    logprobs = -np.log(probabilities[range(N), y]) # loss of each instance\n","    loss = np.sum(logprobs) / N # average loss\n","    \n","    # Compute the gradient\n","    derivative = probabilities\n","    derivative[range(N), y] -= 1 # Subtract one from the entries that correspons to the true class\n","    derivative /= N \n","    \n","    # Backpropagate the gradient to the weights W\n","    dW = X.T.dot(derivative)\n","    \n","    return loss, dW\n","\n","class LinearClassifier(object):\n","\n","    def __init__(self):\n","        self.W = None\n","\n","    def train(self, X, y, learning_rate=1e-3, num_iters=100,\n","            batch_size=200, verbose=False):\n","        \"\"\"\n","        Train this linear classifier using stochastic gradient descent.\n","\n","        Inputs:\n","        - X: A numpy array of shape (N, D) containing training data; there are N\n","            training samples each of dimension D.\n","        - y: A numpy array of shape (N,) containing training labels; y[i] = c\n","            means that X[i] has label 0 <= c < C for C classes.\n","        - learning_rate: (float) learning rate for optimization.\n","        - num_iters: (integer) number of steps to take when optimizing\n","        - batch_size: (integer) number of training examples to use at each step.\n","        - verbose: (boolean) If true, print progress during optimization.\n","\n","        Outputs:\n","        A list containing the value of the loss function at each training iteration.\n","        \"\"\"\n","        num_train, dim = X.shape\n","        num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n","        if self.W is None:\n","            # lazily initialize W\n","            self.W = 0.001 * np.random.randn(dim, num_classes)\n","\n","        # Run stochastic gradient descent to optimize W\n","        loss_history = []\n","        for it in range(num_iters):\n","            #########################################################################\n","            # TODO:                                                                 #\n","            # Sample batch_size elements from the training data and their           #\n","            # corresponding labels to use in this round of gradient descent.        #\n","            # Store the data in X_batch and their corresponding labels in           #\n","            # y_batch; after sampling X_batch should have shape (dim, batch_size)   #\n","            # and y_batch should have shape (batch_size,)                           #\n","            #                                                                       #\n","            # Hint: Use np.random.choice to generate indices. Sampling with         #\n","            # replacement is faster than sampling without replacement.              #\n","            #########################################################################\n","            indices = np.random.choice(num_train, batch_size, replace=True)\n","            X_batch = X[indices]\n","            y_batch = y[indices]\n","            #########################################################################\n","            #                       END OF YOUR CODE                                #\n","            #########################################################################\n","\n","            # evaluate loss and gradient\n","            loss, grad = self.loss(X_batch, y_batch)\n","            loss_history.append(loss)\n","\n","            # perform parameter update\n","            #########################################################################\n","            # TODO:                                                                 #\n","            # Update the weights using the gradient and the learning rate.          #\n","            #########################################################################\n","            self.W -= learning_rate*grad\n","            #########################################################################\n","            #                       END OF YOUR CODE                                #\n","            #########################################################################\n","\n","            if verbose and it % 100 == 0:\n","                print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n","\n","        return loss_history\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Use the trained weights of this linear classifier to predict labels for\n","        data points.\n","\n","        Inputs:\n","        - X: A numpy array of shape (N, D) containing training data; there are N\n","        training samples each of dimension D.\n","\n","        Returns:\n","        - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n","        array of length N, and each element is an integer giving the predicted\n","        class.\n","        \"\"\"\n","        y_pred = np.zeros(X.shape[0])\n","        ###########################################################################\n","        # TODO:                                                                   #\n","        # Implement this method. Store the predicted labels in y_pred.            #\n","        ###########################################################################\n","        class_scores = X.dot(self.W)\n","        y_pred = np.argmax(class_scores, axis=1)\n","        ###########################################################################\n","        #                           END OF YOUR CODE                              #\n","        ###########################################################################\n","        return y_pred\n","  \n","    def loss(self, X_batch, y_batch):\n","        \"\"\"\n","        Compute the loss function and its derivative. \n","        Subclasses will override this.\n","\n","        Inputs:\n","        - X_batch: A numpy array of shape (N, D) containing a minibatch of N\n","            data points; each point has dimension D.\n","        - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n","\n","        Returns: A tuple containing:\n","        - loss as a single float\n","        - gradient with respect to self.W; an array of the same shape as W\n","        \"\"\"\n","        return softmax_loss_vectorized(self.W, X_batch, y_batch)\n","        \n","\n","\n","class Softmax(LinearClassifier):\n","    \"\"\" A subclass that uses the Softmax + Cross-entropy loss function \"\"\"\n","\n","    def loss(self, X_batch, y_batch):\n","        return softmax_loss_vectorized(self.W, X_batch, y_batch)"]},{"cell_type":"markdown","metadata":{"id":"6chaH4G-Vfms"},"source":["**TODO:** Copy the model you created from the Pytorch Notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mSTfKTHEJBhy"},"outputs":[],"source":["class Net(torch.nn.Module):\n","    def __init__(self, n_feature, n_hidden, n_output):\n","        super(Net, self).__init__()\n","        \n","        ################################################################################\n","        # TODO:                                                                        #\n","        # Define 2 or more different layers of the neural network                      #\n","        ################################################################################\n","        self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n","        self.conv2 = torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n","        self.conv3 = torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n","        \n","        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n","        \n","        self.fc1 = torch.nn.Linear(64 * 4 * 4, 128)\n","        self.fc2 = torch.nn.Linear(128, n_output)\n","        \n","        # Dropout layer (to prevent overfitting)\n","        self.dropout = torch.nn.Dropout(0.5)\n","        ################################################################################\n","        #                              END OF YOUR CODE                                #\n","        ################################################################################\n","\n","\n","    def forward(self, x):\n","        x = x.view(x.size(0),-1)\n","        ################################################################################\n","        # TODO:                                                                        #\n","        # Set up the forward pass that the input data will go through.                 #\n","        # A good activation function betweent the layers is a ReLu function.           #\n","        ################################################################################\n","        # Apply conv follow by relu then pool\n","        x = x.view(-1, 3, 32, 32)\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = self.pool(F.relu(self.conv3(x)))\n","        x = x.view(-1, 64 * 4 * 4)\n","        x = self.dropout(x)\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        x = self.fc2(x)  # No activation function, as this will be handled by the loss function for multi-class classification\n","        \n","        ################################################################################\n","        #                              END OF YOUR CODE                                #\n","        ################################################################################\n","        return x\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_UUbNTUAVsos"},"source":["**TODO**: Follow the instructions in each of the following methods. **Note that these methods should return a 1-D array of size N where N is the number of data samples. The values should be the predicted classes [0,...,9].**\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bEKafMuaI4By"},"outputs":[],"source":["def predict_usingPytorch(X):\n","    #########################################################################\n","    # TODO:                                                                 #\n","    # - Create your model                                                   #\n","    # - Load your saved model                                               #\n","    # - Do the operation required to get the predictions                    #\n","    # - Return predictions in a numpy array (hint: return \"argmax\")         #\n","    #########################################################################\n","    net = Net(n_feature=3072, n_hidden=100, n_output=10)\n","    checkpoint = torch.load(\"drive/MyDrive/Colab Notebooks/linearClassifier_pytorch.ckpt\")\n","    net.load_state_dict(checkpoint)\n","\n","    net.eval()\n","    \n","    with torch.no_grad():\n","        logits = net(X)  # Get the raw model outputs\n","        y_pred = torch.argmax(logits, dim=1)\n","    #########################################################################\n","    #                       END OF YOUR CODE                                #\n","    #########################################################################\n","    return y_pred.numpy()\n","\n","def predict_usingSoftmax(X):\n","    # Load your saved weights\n","    with open('drive/MyDrive/Colab Notebooks/softmax_weights.pkl', 'rb') as f:\n","        W = pickle.load(f)\n","    \n","    # Convert X to a PyTorch tensor if it's not already (assuming X is a NumPy array)\n","    if isinstance(X, np.ndarray):\n","        X = torch.from_numpy(X).float()\n","    \n","    # If your weights are in a NumPy array, convert them to a PyTorch tensor\n","    if isinstance(W, np.ndarray):\n","        W = torch.from_numpy(W).float()\n","    \n","    # Ensure W is transposed if necessary, depending on how your data and weights align\n","    # logits = torch.matmul(X, W.T) if your model expects features as row vectors\n","    logits = torch.matmul(X, W)\n","    \n","    # Apply softmax to obtain the probabilities for each class\n","    probabilities = torch.softmax(logits, dim=1)\n","    \n","    # Get the predicted class indices\n","    _, y_pred = torch.max(probabilities, 1)\n","    \n","    # Return predictions as a numpy array\n","    return y_pred.numpy()"]},{"cell_type":"markdown","metadata":{"id":"q8dM8fj39OBP"},"source":["This method loads the test dataset to evaluate the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"400u4eZNJAZq"},"outputs":[],"source":["## Read DATA\n","def load_pickle(f):\n","    version = platform.python_version_tuple()\n","    if version[0] == '2':\n","        return  pickle.load(f)\n","    elif version[0] == '3':\n","        return  pickle.load(f, encoding='latin1')\n","    raise ValueError(\"invalid python version: {}\".format(version))\n","\n","def load_CIFAR_batch(filename):\n","  \"\"\" load single batch of cifar \"\"\"\n","  with open(filename, 'rb') as f:\n","    datadict = load_pickle(f)\n","    X = datadict['data']\n","    Y = datadict['labels']\n","    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n","    Y = np.array(Y)\n","    return X, Y\n","test_filename = path\n","X,Y = load_CIFAR_batch(test_filename)"]},{"cell_type":"markdown","metadata":{"id":"AJ3mBYnx9TIe"},"source":["This code snippet prepares the data for the different models. If you modify data manipulation in your notebooks, make sure to include them here. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IEmU5KnwJPBY"},"outputs":[],"source":["## Data Manipulation\n","\n","mean = np.array([0.4914, 0.4822, 0.4465])\n","std = np.array([0.2023, 0.1994, 0.2010])\n","X = np.divide(np.subtract( X/255 , mean[np.newaxis,np.newaxis,:]), std[np.newaxis,np.newaxis,:])\n","\n","X_pytorch = torch.Tensor(np.moveaxis(X,-1,1))\n","X_softmax = np.reshape(X, (X.shape[0], -1))\n","X_softmax = np.hstack([X_softmax, np.ones((X_softmax.shape[0], 1))])\n"]},{"cell_type":"markdown","metadata":{"id":"O2nQbKPL9c3G"},"source":["Runs evaluation on the Pytorch and softmax model. **Be careful that *prediction_pytorch* and *prediction_softmax* are 1-D array of size N where N is the number of data samples. The values should be the predicted class [0,...,9]**\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VKFPhm1wJjDv"},"outputs":[],"source":["## Run Prediction\n","prediction_pytorch = predict_usingPytorch(X_pytorch)\n","prediction_softmax = predict_usingSoftmax(X_softmax)\n","\n","## Run Evaluation\n","acc_softmax = sum(prediction_softmax == Y)/len(X)\n","acc_pytorch = sum(prediction_pytorch == Y)/len(X)\n","print(\"Softmax= %f ... Pytorch= %f\"%(acc_softmax, acc_pytorch))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qroI8swROjZf"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Evaluator.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
